# -*- coding: utf-8 -*-
"""test_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mdOop6PZMlR8r8WdmMeYpgcV2QsX9fM9
"""

import time
def train1(train1_indc,dataloader1,gf1):
    GAT = GATv2()
    GAT.train()

    for i in range(len(train1_indc)):  # Iterate in batches over the train1ing dataset.
         id = train1_indc[i]
         data1 = dataloader1[id]
         
        #  gf1 = gf1[id]
         data1.y = torch.Tensor(data1.y)
         out = GAT(data1.x.float().T, data1.edge_index,data1.edge_attr, data1.batch, gf1[id])  

         loss = criterion(out.float(), data1.y.float()) # Compute the loss.
         loss.backward()  # Derive gradients.
         optimizer.step()  # Update parameters based on gradients.
         optimizer.zero_grad()  # Clear gradients.

def test1(indc1,dataloader1, gf1):
     t1=time.time()
     GAT = GATv2()
     GAT.eval()

     correct = 0
     times=[]
     for i in range(len(indc1)):  # Iterate in batches over the training dataset.
         id = indc1[i]
         data1 = batch[id]
         out = GAT(data1.x.float().T, data1.edge_index,data1.edge_attr, data1.batch, gf1[id])  
         correct += int(((out>0.5) == data1.y).sum())  # Check against ground-truth labels.
         t2=time.time()
         print(t2-t1)
         print(len(data1.x))
        #  times.append[t2-t1]
     return correct / len(indc1),t2-t1  # Derive ratio of correct predictions.